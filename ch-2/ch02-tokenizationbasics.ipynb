{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5368c8a8-0950-4ef2-a213-92aa6a24f300",
   "metadata": {},
   "source": [
    "## Building a basic tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "804756eb-7af6-4655-83a3-9eda60e47f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version:  2.4.1\n",
      "tiktoken version:  0.7.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version: \", version(\"torch\"))\n",
    "print(\"tiktoken version: \", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c8e0ee6-1bee-4b9d-b7bd-9c7eb64fcce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing around with the verdict by edith wharton\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a7f060e-0667-4113-9b0c-c19ca284ad06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9848bf42-ceec-4775-b828-f21570a1d6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "# Some basic tokenization examples\n",
    "# Basic tokenization - space separated\n",
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af3df8bb-3fa3-4d6c-934a-fc7e245a0428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n",
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "# Basic tokenization - space and punctuation separated\n",
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)\n",
    "\n",
    "# this will create groups of empty string too, which we want to skip\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5d3aed4-d4f9-4cbc-afd4-6683dd4a078f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'text', '?']\n"
     ]
    }
   ],
   "source": [
    "# tokenization above, handles only basic punctuation, make it more general\n",
    "\n",
    "text = \"Hello, world. Is this-- a text?\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33860be2-862d-4bd4-9938-a0bb9317e6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n",
      "Total tokens:  4690\n"
     ]
    }
   ],
   "source": [
    "# the regex that we've arrived at seems pretty general\n",
    "# time to apply it to raw text\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])\n",
    "\n",
    "print(\"Total tokens: \", len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ee446ca-e558-4ac6-919e-2c55893dee04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "# build a vocabulary from these set of tokens\n",
    "\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)\n",
    "\n",
    "# num unique words - 1130\n",
    "# everytime I get a word from the dict - it will be mapped to the same int on the input side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51ecf2d9-890e-41a5-bfce-7137ea2fc8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize a vocabulary to keep track of all words\n",
    "vocab = {token:integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "317cb3ed-d9f3-4b82-8094-abcb079e3ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! 0\n",
      "\" 1\n",
      "' 2\n",
      "( 3\n",
      ") 4\n",
      ", 5\n",
      "-- 6\n",
      ". 7\n",
      ": 8\n",
      "; 9\n",
      "? 10\n",
      "A 11\n",
      "Ah 12\n",
      "Among 13\n",
      "And 14\n",
      "Are 15\n",
      "Arrt 16\n",
      "As 17\n",
      "At 18\n",
      "Be 19\n",
      "Begin 20\n",
      "Burlington 21\n",
      "But 22\n",
      "By 23\n",
      "Carlo 24\n",
      "Chicago 25\n",
      "Claude 26\n",
      "Come 27\n",
      "Croft 28\n",
      "Destroyed 29\n",
      "Devonshire 30\n",
      "Don 31\n",
      "Dubarry 32\n",
      "Emperors 33\n",
      "Florence 34\n",
      "For 35\n",
      "Gallery 36\n",
      "Gideon 37\n",
      "Gisburn 38\n",
      "Gisburns 39\n",
      "Grafton 40\n",
      "Greek 41\n",
      "Grindle 42\n",
      "Grindles 43\n",
      "HAD 44\n",
      "Had 45\n",
      "Hang 46\n",
      "Has 47\n",
      "He 48\n",
      "Her 49\n",
      "Hermia 50\n"
     ]
    }
   ],
   "source": [
    "for key, val in vocab.items():\n",
    "    print(key, val)\n",
    "    if val >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87448500-fb50-4d46-a59d-413308f03475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's put it all together into a class\n",
    "\n",
    "\n",
    "class SimpleTokenizerV1():\n",
    "    def __init__(self, vocab):\n",
    "\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5290aa77-951c-4fd7-9776-cb74f3e8cc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "# let's test the tokenizer we've defined above\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c8fd86e-8925-4b9d-95b6-964437cecea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17e9cf97-2b63-48e7-96fe-4739151afa3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple test to encode and decode, expect the same input text as output\n",
    "\n",
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c296c1a0-0b62-4796-adbd-0dbe17da5653",
   "metadata": {},
   "source": [
    "## Note about special tokens and GPT2 Tokenization\n",
    "\n",
    "- Many LLMs use additional tokens <bos> <eos> <pad> to provide LLMs with additional context\n",
    "- GPT2 uses <|endoftext|> as the only special token - one for padding and the second one for denoting end of sequence.\n",
    "- Usually <|endoftext|> is used to concetante 2 unrelated pieces of text\n",
    "- Also, GPT2 uses byte pair encoding, which breaks down the text at a subword level making sure that nothing is out of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb78356e-efef-4b94-a87f-aa1847322c08",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m SimpleTokenizerV1(vocab)\n\u001b[1;32m      2\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like tea. Is this-- a test?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     12\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m     13\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[0;32m---> 14\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m     13\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[0;32m---> 14\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"Hello, do you like tea. Is this-- a test?\"\n",
    "tokenizer.encode(text)\n",
    "\n",
    "# Here we didn't have code for managing out of vocabulary text\n",
    "# resulting in an error - when we encounter such text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "843c9ffb-a2fa-4b2a-8244-07ea44d776df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's include special tokens to handle these situations\n",
    "\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "# update the original vocabulary\n",
    "vocab = {s:i for i,s in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8bc3b69-be94-43ad-b038-a12b41899367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# updated len of vocab\n",
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c63e344-1193-42bd-a7b6-074def4cd3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "younger 1127\n",
      "your 1128\n",
      "yourself 1129\n",
      "<|endoftext|> 1130\n",
      "<|unk|> 1131\n"
     ]
    }
   ],
   "source": [
    "# let's peek into the ids assigned to the vocab\n",
    "for i, item in list(vocab.items())[-5:]:\n",
    "    print(i, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b81030ce-49e6-42c1-ae8e-bd6a81266382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the tokenizer class created earlier with support for\n",
    "# handling tokens out of the vocabulary\n",
    "\n",
    "class SimpleTokenizerV2():\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "455922ae-d65f-4650-9e62-bff9dbb4d8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "# Let's try out the modified tokenizer\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acfedbc3-71a9-4ec2-995d-5f5796da60e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)\n",
    "\n",
    "# hello, palace - 1131\n",
    "# <|endoftext|> - 1130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0781b5b-6c02-4955-ba81-bd9b21304fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7224fdd-dbe5-4448-a7ed-ed2b5070ec0f",
   "metadata": {},
   "source": [
    "## Playing around with BytePairEncoding\n",
    "\n",
    "- GPT2 uses a BPE as its tokenizer\n",
    "- Allows the model to breakdown, words not in it's predefined vocabulary into smaller words - subwords or individual characters, enabling it to handle out of vocabulary words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54552890-a88c-45ad-bfce-6e4684197ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version:  0.7.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version: \", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1c8fdad-26f0-4cbd-bcef-0468aba07701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the tokenizer to use BPE\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fffd8d5b-1608-46ed-9c67-7f610cafc526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)\n",
    "\n",
    "# allowed special maps the given token in the vocabulary to a given integer\n",
    "# otherwise if we disable allowed_special it will throw an error, further more it will break it down into subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fec29801-f839-419c-a95b-f552f75ce08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)\n",
    "\n",
    "# we've got the same text back as the original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e0c74b-5e98-409b-add7-b685ab160076",
   "metadata": {},
   "source": [
    "## Data Sampling with a Sliding Window\n",
    "\n",
    "- We train LLMs one word at a time, so we want to prepare the training data accordingly where the next word in the sequence represents the target to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b504482-f553-4a5f-91cd-f6d4229ad644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "135399c6-9daa-4c0a-85aa-c3f017324e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "enc_sample = enc_text[50:]\n",
    "\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(\"x: \", x)\n",
    "print(\"y:     \", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fab778a0-5787-4a27-b10c-23fca93b89e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      " and ---->  established\n",
      "\n",
      "[290, 4920] ----> 2241\n",
      " and established ---->  himself\n",
      "\n",
      "[290, 4920, 2241] ----> 287\n",
      " and established himself ---->  in\n",
      "\n",
      "[290, 4920, 2241, 287] ----> 257\n",
      " and established himself in ---->  a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# visualizing how the prediction would look one by one\n",
    "\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, '---->', desired)\n",
    "    print(tokenizer.decode(context), '---->', tokenizer.decode([desired]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5177bd31-04de-4fa7-9918-e27efa7f0334",
   "metadata": {},
   "source": [
    "## Creating a simple dataloader to create the dataset above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31d8463d-8811-4e17-b4c4-9eff70de5935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version:  2.4.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"torch version: \", version(\"torch\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ef99e96-d267-4cb4-9fc6-8065c26d8c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we give samples from the dataloader in the format which we intend to train\n",
    "# embedding - label\n",
    "\n",
    "# here we are passing\n",
    "# context -> label [extra tokens]\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        \n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i+1: i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f1791809-2444-461c-846c-874589d5d285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                        stride=128, shuffle=True, drop_last=True,\n",
    "                        num_workers=0):\n",
    "    \n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5d9a08a-c835-479d-a125-71cbb3e6dba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,\n",
    "    batch_size=1,\n",
    "    max_length=4,\n",
    "    stride=3,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7733de7a-2e9a-41c3-93a8-e0375924a72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1464, 1807, 3619,  402]]), tensor([[1807, 3619,  402,  271]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46d4ed9d-8f59-4d05-ae6f-e3f77ee72820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  402,   271, 10899,  2138]]), tensor([[  271, 10899,  2138,   257]])]\n",
      "\n",
      "[tensor([[ 2138,   257,  7026, 15632]]), tensor([[  257,  7026, 15632,   438]])]\n",
      "\n",
      "[tensor([[15632,   438,  2016,   257]]), tensor([[ 438, 2016,  257,  922]])]\n",
      "\n",
      "[tensor([[ 257,  922, 5891, 1576]]), tensor([[ 922, 5891, 1576,  438]])]\n",
      "\n",
      "[tensor([[1576,  438,  568,  340]]), tensor([[438, 568, 340, 373]])]\n",
      "\n",
      "[tensor([[ 340,  373,  645, 1049]]), tensor([[ 373,  645, 1049, 5975]])]\n",
      "\n",
      "[tensor([[1049, 5975,  284,  502]]), tensor([[5975,  284,  502,  284]])]\n",
      "\n",
      "[tensor([[ 502,  284, 3285,  326]]), tensor([[ 284, 3285,  326,   11]])]\n",
      "\n",
      "[tensor([[326,  11, 287, 262]]), tensor([[  11,  287,  262, 6001]])]\n",
      "\n",
      "[tensor([[ 262, 6001,  286,  465]]), tensor([[ 6001,   286,   465, 13476]])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    curr_batch = next(data_iter)\n",
    "    print(curr_batch)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6434a793-7dac-46a8-962c-900fd80e662c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Targets: \n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=4,\n",
    "    stride=4, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, target = next(data_iter)\n",
    "\n",
    "print(\"Inputs: \\n\", inputs)\n",
    "print(\"Targets: \\n\", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f05d1c-51b0-4d67-ada8-b35ce19ffc7b",
   "metadata": {},
   "source": [
    "## Creating token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f959fbd-32c5-432d-92ea-28b1c18c7cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "58467e21-70b2-4409-b2de-26f7b4e0b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d461152-61bc-460e-b937-a93437790f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight)\n",
    "\n",
    "# embedding layer is just a lookup - where we convet specific ids to specific continuous numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b61b525e-4203-40a9-9b8d-573fabc8cb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 :  tensor([ 1.2753, -0.2010, -0.1606], grad_fn=<EmbeddingBackward0>)\n",
      "3 :  tensor([-0.4015,  0.9666, -1.1481], grad_fn=<EmbeddingBackward0>)\n",
      "5 :  tensor([-2.8400, -0.7849, -1.4096], grad_fn=<EmbeddingBackward0>)\n",
      "1 :  tensor([0.9178, 1.5810, 1.3010], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Get the embedding for each of the inde\n",
    "\n",
    "print(\"2 : \", embedding_layer(torch.tensor(2)))\n",
    "print(\"3 : \", embedding_layer(torch.tensor(3)))\n",
    "print(\"5 : \", embedding_layer(torch.tensor(5)))\n",
    "print(\"1 : \", embedding_layer(torch.tensor(1)))\n",
    "\n",
    "# embedding layer is basically a lookup\n",
    "# let's say that I wanted a 512 size one hot vector for the number 3, \n",
    "# then the position at which the numbers are one hot encoded would change according the ouptut vector dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aefab4-5252-40ac-855f-a2cbf7e0093c",
   "metadata": {},
   "source": [
    "### Creating token embeddings\n",
    "\n",
    "- embedding layer converts ids into vector representation irrespective of where they are located.\n",
    "- positional embedding is combined with the token embedding vector to form the input embedding to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8c6bceb-4b8f-4b14-b02e-2ac3644db792",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 505257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c6e3c46c-4ef3-43b5-9105-b8a555676a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, target = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dce116ea-54c9-421e-a57a-20925a6dccb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Ids \n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape: \n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token Ids \\n\", inputs)\n",
    "print(\"\\nInputs shape: \\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "03343fb3-50fa-4492-9540-23de7d3da8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# let's create token embeddings\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "32cac382-36a3-4b65-98b2-2e36bdae817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt2 uses absolute position embeddings, so we create another embedding layers\n",
    "\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "be28fe3b-780f-4528-8776-981ab87a70ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e99a974c-6b41-48b2-94b5-c74265f81226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3af072-1ed3-4adc-a5d4-68d945ee8896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
